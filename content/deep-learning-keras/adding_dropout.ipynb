{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Adding Dropout    \n",
    "Slug: adding_dropout    \n",
    "Summary: How to add dropout to a neural network model in Python.    \n",
    "Date: 2017-09-25 12:00  \n",
    "Category: Deep Learning - Keras  \n",
    "Tags: Basics\n",
    "Authors: Kabir Khan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Dropout?\n",
    "\n",
    "Dropout is a state-of-the-art regularization technique for deep neural networks. It is a mask over a particular layer of a network to remove certain nodes and their connections from a training pass.\n",
    "\n",
    "### **e.g.** \n",
    "\n",
    "If you apply dropout with a probability of 0.2 to a layer, then for each node in that layer there is 0.2 chance of removing it from training. \n",
    "\n",
    "For each iteration, a different subset of your network is trained creating an ensemble effect. This technique is shown to reduce overfitting and is used in most network architectures today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn2/dropout.jpeg\">\n",
    "> Image Source: http://cs231n.github.io/neural-networks-2/#reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDB Movie Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 36s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')\n",
    "test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "    # Compile neural network\n",
    "    model.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', # Adam Optimizer\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "    \n",
    "    # Train neural network\n",
    "    history = model.fit(train_features, # Features\n",
    "                      train_target, # Target vector\n",
    "                      epochs=10, # Number of epochs\n",
    "                      verbose=1, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(test_features, test_target)) # Data for evaluation\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We'll start with a standard network architecture that doesn't use Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function for input layer\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 62us/step - loss: 0.4306 - acc: 0.7998 - val_loss: 0.3347 - val_acc: 0.8580\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 1s 42us/step - loss: 0.3220 - acc: 0.8655 - val_loss: 0.3288 - val_acc: 0.8596\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 1s 44us/step - loss: 0.3100 - acc: 0.8716 - val_loss: 0.3346 - val_acc: 0.8578\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.3020 - acc: 0.8742 - val_loss: 0.3273 - val_acc: 0.8596\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 1s 42us/step - loss: 0.2910 - acc: 0.8778 - val_loss: 0.3318 - val_acc: 0.8575\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.2773 - acc: 0.8852 - val_loss: 0.3311 - val_acc: 0.8572\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.2638 - acc: 0.8940 - val_loss: 0.3382 - val_acc: 0.8552\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.2482 - acc: 0.8985 - val_loss: 0.3533 - val_acc: 0.8511\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 1s 59us/step - loss: 0.2325 - acc: 0.9069 - val_loss: 0.3646 - val_acc: 0.8490\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.2140 - acc: 0.9160 - val_loss: 0.3940 - val_acc: 0.8450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11643df98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture With Dropout\n",
    "\n",
    "In Keras, we can implement dropout using the convenient `Dropout` layer into our network architecture. Each `Dropout` layer will drop a user-defined hyperparameter of units in the previous layer every batch. \n",
    "\n",
    "Remember in Keras the input layer is assumed to be the first layer and not added using the `add` method. Therefore, if we want to add dropout to the input layer, the layer we add in our model is a dropout layer. This layer contains both the proportion of the input layer's units to drop `0.2` and `input_shape` defining the shape of the observation data. We add `Dropout` layers with `0.5` to each subsequent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start neural network with dropout\n",
    "network_dropout = models.Sequential()\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "network_dropout.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network_dropout.add(layers.Dense(units=16, activation='relu'))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network_dropout.add(layers.Dense(units=16, activation='relu'))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network_dropout.add(layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 78us/step - loss: 0.6368 - acc: 0.6172 - val_loss: 0.4679 - val_acc: 0.8290\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.4896 - acc: 0.7714 - val_loss: 0.3674 - val_acc: 0.8484\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 63us/step - loss: 0.4348 - acc: 0.8100 - val_loss: 0.3393 - val_acc: 0.8579\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.4130 - acc: 0.8181 - val_loss: 0.3261 - val_acc: 0.8603\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.4034 - acc: 0.8222 - val_loss: 0.3214 - val_acc: 0.8623\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 83us/step - loss: 0.3980 - acc: 0.8232 - val_loss: 0.3262 - val_acc: 0.8612\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.3926 - acc: 0.8284 - val_loss: 0.3220 - val_acc: 0.8607\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.3889 - acc: 0.8263 - val_loss: 0.3233 - val_acc: 0.8588\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.3814 - acc: 0.8350 - val_loss: 0.3216 - val_acc: 0.8584\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 66us/step - loss: 0.3804 - acc: 0.8323 - val_loss: 0.3209 - val_acc: 0.8604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1170d7fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(network_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can see that without dropout we'll get a much higher training accuracy over 10 epochs. However, our validation accuracy starts to decline. With dropout, our network's training accuracy regularizes and we have a consistently higher validation accuracy.\n",
    "\n",
    "**Validation Accuracy Comparison**:\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <th> **Network** </th>\n",
    "            <th> **Network With Dropout** </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td> **User stayed in the course** </td>\n",
    "            <td> 84.5% </td>\n",
    "            <td> 86.04% </td>\n",
    "        </tr>\n",
    "    </table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
